{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Digit Recognizer\n",
    "\n",
    "train.csv:\n",
    "label,pixel0,pixel1,pixel2,pixel3,pixel4,pixel5,pixel6,pixel7,pixel8,pixel9,pixel10,pixel11,pixel12,pixel13,pixel14,pixel15,pixel16,pixel17,pixel18,pixel19,pixel20,pixel21,pixel22,pixel23,pixel24,pixel25,pixel26,pixel27,pixel28,pixel29,pixel30,pixel31,pixel32,pixel33,pixel34,pixel35,pixel36,pixel37,pixel38,pixel39,pixel40,pixel41,pixel42,pixel43,pixel44,pixel45,pixel46,pixel47,pixel48,pixel49,pixel50,pixel51,pixel52,pixel53,pixel54,pixel55,pixel56,pixel57,pixel58,pixel59,pixel60,pixel61,pixel62,pixel63,pixel64,pixel65,pixel66,pixel67,pixel68,pixel69,pixel70,pixel71,pixel72,pixel73,pixel74,pixel75,pixel76,pixel77,pixel78,pixel79,pixel80,pixel81,pixel82,pixel83,pixel84,pixel85,pixel86,pixel87,pixel88,pixel89,pixel90,pixel91,pixel92,pixel93,pixel94,pixel95,pixel96,pixel97,pixel98,pixel99,pixel100,pixel101,pixel102,pixel103,pixel104,pixel105,pixel106,pixel107,pixel108,pixel109,pixel110,pixel111,pixel112,pixel113,pixel114,pixel115,pixel116,pixel117,pixel118,pixel119,pixel120,pixel121,pixel122,pixel123,pixel124,pixel125,pixel126,pixel127,pixel128,pixel129,pixel130,pixel131,pixel132,pixel133,pixel134,pixel135,pixel136,pixel137,pixel138,pixel139,pixel140,pixel141,pixel142,pixel143,pixel144,pixel145,pixel146,pixel147,pixel148,pixel149,pixel150,pixel151,pixel152,pixel153,pixel154,pixel155,pixel156,pixel157,pixel158,pixel159,pixel160,pixel161,pixel162,pixel163,pixel164,pixel165,pixel166,pixel167,pixel168,pixel169,pixel170,pixel171,pixel172,pixel173,pixel174,pixel175,pixel176,pixel177,pixel178,pixel179,pixel180,pixel181,pixel182,pixel183,pixel184,pixel185,pixel186,pixel187,pixel188,pixel189,pixel190,pixel191,pixel192,pixel193,pixel194,pixel195,pixel196,pixel197,pixel198,pixel199,pixel200,pixel201,pixel202,pixel203,pixel204,pixel205,pixel206,pixel207,pixel208,pixel209,pixel210,pixel211,pixel212,pixel213,pixel214,pixel215,pixel216,pixel217,pixel218,pixel219,pixel220,pixel221,pixel222,pixel223,pixel224,pixel225,pixel226,pixel227,pixel228,pixel229,pixel230,pixel231,pixel232,pixel233,pixel234,pixel235,pixel236,pixel237,pixel238,pixel239,pixel240,pixel241,pixel242,pixel243,pixel244,pixel245,pixel246,pixel247,pixel248,pixel249,pixel250,pixel251,pixel252,pixel253,pixel254,pixel255,pixel256,pixel257,pixel258,pixel259,pixel260,pixel261,pixel262,pixel263,pixel264,pixel265,pixel266,pixel267,pixel268,pixel269,pixel270,pixel271,pixel272,pixel273,pixel274,pixel275,pixel276,pixel277,pixel278,pixel279,pixel280,pixel281,pixel282,pixel283,pixel284,pixel285,pixel286,pixel287,pixel288,pixel289,pixel290,pixel291,pixel292,pixel293,pixel294,pixel295,pixel296,pixel297,pixel298,pixel299,pixel300,pixel301,pixel302,pixel303,pixel304,pixel305,pixel306,pixel307,pixel308,pixel309,pixel310,pixel311,pixel312,pixel313,pixel314,pixel315,pixel316,pixel317,pixel318,pixel319,pixel320,pixel321,pixel322,pixel323,pixel324,pixel325,pixel326,pixel327,pixel328,pixel329,pixel330,pixel331,pixel332,pixel333,pixel334,pixel335,pixel336,pixel337,pixel338,pixel339,pixel340,pixel341,pixel342,pixel343,pixel344,pixel345,pixel346,pixel347,pixel348,pixel349,pixel350,pixel351,pixel352,pixel353,pixel354,pixel355,pixel356,pixel357,pixel358,pixel359,pixel360,pixel361,pixel362,pixel363,pixel364,pixel365,pixel366,pixel367,pixel368,pixel369,pixel370,pixel371,pixel372,pixel373,pixel374,pixel375,pixel376,pixel377,pixel378,pixel379,pixel380,pixel381,pixel382,pixel383,pixel384,pixel385,pixel386,pixel387,pixel388,pixel389,pixel390,pixel391,pixel392,pixel393,pixel394,pixel395,pixel396,pixel397,pixel398,pixel399,pixel400,pixel401,pixel402,pixel403,pixel404,pixel405,pixel406,pixel407,pixel408,pixel409,pixel410,pixel411,pixel412,pixel413,pixel414,pixel415,pixel416,pixel417,pixel418,pixel419,pixel420,pixel421,pixel422,pixel423,pixel424,pixel425,pixel426,pixel427,pixel428,pixel429,pixel430,pixel431,pixel432,pixel433,pixel434,pixel435,pixel436,pixel437,pixel438,pixel439,pixel440,pixel441,pixel442,pixel443,pixel444,pixel445,pixel446,pixel447,pixel448,pixel449,pixel450,pixel451,pixel452,pixel453,pixel454,pixel455,pixel456,pixel457,pixel458,pixel459,pixel460,pixel461,pixel462,pixel463,pixel464,pixel465,pixel466,pixel467,pixel468,pixel469,pixel470,pixel471,pixel472,pixel473,pixel474,pixel475,pixel476,pixel477,pixel478,pixel479,pixel480,pixel481,pixel482,pixel483,pixel484,pixel485,pixel486,pixel487,pixel488,pixel489,pixel490,pixel491,pixel492,pixel493,pixel494,pixel495,pixel496,pixel497,pixel498,pixel499,pixel500,pixel501,pixel502,pixel503,pixel504,pixel505,pixel506,pixel507,pixel508,pixel509,pixel510,pixel511,pixel512,pixel513,pixel514,pixel515,pixel516,pixel517,pixel518,pixel519,pixel520,pixel521,pixel522,pixel523,pixel524,pixel525,pixel526,pixel527,pixel528,pixel529,pixel530,pixel531,pixel532,pixel533,pixel534,pixel535,pixel536,pixel537,pixel538,pixel539,pixel540,pixel541,pixel542,pixel543,pixel544,pixel545,pixel546,pixel547,pixel548,pixel549,pixel550,pixel551,pixel552,pixel553,pixel554,pixel555,pixel556,pixel557,pixel558,pixel559,pixel560,pixel561,pixel562,pixel563,pixel564,pixel565,pixel566,pixel567,pixel568,pixel569,pixel570,pixel571,pixel572,pixel573,pixel574,pixel575,pixel576,pixel577,pixel578,pixel579,pixel580,pixel581,pixel582,pixel583,pixel584,pixel585,pixel586,pixel587,pixel588,pixel589,pixel590,pixel591,pixel592,pixel593,pixel594,pixel595,pixel596,pixel597,pixel598,pixel599,pixel600,pixel601,pixel602,pixel603,pixel604,pixel605,pixel606,pixel607,pixel608,pixel609,pixel610,pixel611,pixel612,pixel613,pixel614,pixel615,pixel616,pixel617,pixel618,pixel619,pixel620,pixel621,pixel622,pixel623,pixel624,pixel625,pixel626,pixel627,pixel628,pixel629,pixel630,pixel631,pixel632,pixel633,pixel634,pixel635,pixel636,pixel637,pixel638,pixel639,pixel640,pixel641,pixel642,pixel643,pixel644,pixel645,pixel646,pixel647,pixel648,pixel649,pixel650,pixel651,pixel652,pixel653,pixel654,pixel655,pixel656,pixel657,pixel658,pixel659,pixel660,pixel661,pixel662,pixel663,pixel664,pixel665,pixel666,pixel667,pixel668,pixel669,pixel670,pixel671,pixel672,pixel673,pixel674,pixel675,pixel676,pixel677,pixel678,pixel679,pixel680,pixel681,pixel682,pixel683,pixel684,pixel685,pixel686,pixel687,pixel688,pixel689,pixel690,pixel691,pixel692,pixel693,pixel694,pixel695,pixel696,pixel697,pixel698,pixel699,pixel700,pixel701,pixel702,pixel703,pixel704,pixel705,pixel706,pixel707,pixel708,pixel709,pixel710,pixel711,pixel712,pixel713,pixel714,pixel715,pixel716,pixel717,pixel718,pixel719,pixel720,pixel721,pixel722,pixel723,pixel724,pixel725,pixel726,pixel727,pixel728,pixel729,pixel730,pixel731,pixel732,pixel733,pixel734,pixel735,pixel736,pixel737,pixel738,pixel739,pixel740,pixel741,pixel742,pixel743,pixel744,pixel745,pixel746,pixel747,pixel748,pixel749,pixel750,pixel751,pixel752,pixel753,pixel754,pixel755,pixel756,pixel757,pixel758,pixel759,pixel760,pixel761,pixel762,pixel763,pixel764,pixel765,pixel766,pixel767,pixel768,pixel769,pixel770,pixel771,pixel772,pixel773,pixel774,pixel775,pixel776,pixel777,pixel778,pixel779,pixel780,pixel781,pixel782,pixel783\n",
    "\n",
    "test.csv:\n",
    "pixel0,pixel1,pixel2,pixel3,pixel4,pixel5,pixel6,pixel7,pixel8,pixel9,pixel10,pixel11,pixel12,pixel13,pixel14,pixel15,pixel16,pixel17,pixel18,pixel19,pixel20,pixel21,pixel22,pixel23,pixel24,pixel25,pixel26,pixel27,pixel28,pixel29,pixel30,pixel31,pixel32,pixel33,pixel34,pixel35,pixel36,pixel37,pixel38,pixel39,pixel40,pixel41,pixel42,pixel43,pixel44,pixel45,pixel46,pixel47,pixel48,pixel49,pixel50,pixel51,pixel52,pixel53,pixel54,pixel55,pixel56,pixel57,pixel58,pixel59,pixel60,pixel61,pixel62,pixel63,pixel64,pixel65,pixel66,pixel67,pixel68,pixel69,pixel70,pixel71,pixel72,pixel73,pixel74,pixel75,pixel76,pixel77,pixel78,pixel79,pixel80,pixel81,pixel82,pixel83,pixel84,pixel85,pixel86,pixel87,pixel88,pixel89,pixel90,pixel91,pixel92,pixel93,pixel94,pixel95,pixel96,pixel97,pixel98,pixel99,pixel100,pixel101,pixel102,pixel103,pixel104,pixel105,pixel106,pixel107,pixel108,pixel109,pixel110,pixel111,pixel112,pixel113,pixel114,pixel115,pixel116,pixel117,pixel118,pixel119,pixel120,pixel121,pixel122,pixel123,pixel124,pixel125,pixel126,pixel127,pixel128,pixel129,pixel130,pixel131,pixel132,pixel133,pixel134,pixel135,pixel136,pixel137,pixel138,pixel139,pixel140,pixel141,pixel142,pixel143,pixel144,pixel145,pixel146,pixel147,pixel148,pixel149,pixel150,pixel151,pixel152,pixel153,pixel154,pixel155,pixel156,pixel157,pixel158,pixel159,pixel160,pixel161,pixel162,pixel163,pixel164,pixel165,pixel166,pixel167,pixel168,pixel169,pixel170,pixel171,pixel172,pixel173,pixel174,pixel175,pixel176,pixel177,pixel178,pixel179,pixel180,pixel181,pixel182,pixel183,pixel184,pixel185,pixel186,pixel187,pixel188,pixel189,pixel190,pixel191,pixel192,pixel193,pixel194,pixel195,pixel196,pixel197,pixel198,pixel199,pixel200,pixel201,pixel202,pixel203,pixel204,pixel205,pixel206,pixel207,pixel208,pixel209,pixel210,pixel211,pixel212,pixel213,pixel214,pixel215,pixel216,pixel217,pixel218,pixel219,pixel220,pixel221,pixel222,pixel223,pixel224,pixel225,pixel226,pixel227,pixel228,pixel229,pixel230,pixel231,pixel232,pixel233,pixel234,pixel235,pixel236,pixel237,pixel238,pixel239,pixel240,pixel241,pixel242,pixel243,pixel244,pixel245,pixel246,pixel247,pixel248,pixel249,pixel250,pixel251,pixel252,pixel253,pixel254,pixel255,pixel256,pixel257,pixel258,pixel259,pixel260,pixel261,pixel262,pixel263,pixel264,pixel265,pixel266,pixel267,pixel268,pixel269,pixel270,pixel271,pixel272,pixel273,pixel274,pixel275,pixel276,pixel277,pixel278,pixel279,pixel280,pixel281,pixel282,pixel283,pixel284,pixel285,pixel286,pixel287,pixel288,pixel289,pixel290,pixel291,pixel292,pixel293,pixel294,pixel295,pixel296,pixel297,pixel298,pixel299,pixel300,pixel301,pixel302,pixel303,pixel304,pixel305,pixel306,pixel307,pixel308,pixel309,pixel310,pixel311,pixel312,pixel313,pixel314,pixel315,pixel316,pixel317,pixel318,pixel319,pixel320,pixel321,pixel322,pixel323,pixel324,pixel325,pixel326,pixel327,pixel328,pixel329,pixel330,pixel331,pixel332,pixel333,pixel334,pixel335,pixel336,pixel337,pixel338,pixel339,pixel340,pixel341,pixel342,pixel343,pixel344,pixel345,pixel346,pixel347,pixel348,pixel349,pixel350,pixel351,pixel352,pixel353,pixel354,pixel355,pixel356,pixel357,pixel358,pixel359,pixel360,pixel361,pixel362,pixel363,pixel364,pixel365,pixel366,pixel367,pixel368,pixel369,pixel370,pixel371,pixel372,pixel373,pixel374,pixel375,pixel376,pixel377,pixel378,pixel379,pixel380,pixel381,pixel382,pixel383,pixel384,pixel385,pixel386,pixel387,pixel388,pixel389,pixel390,pixel391,pixel392,pixel393,pixel394,pixel395,pixel396,pixel397,pixel398,pixel399,pixel400,pixel401,pixel402,pixel403,pixel404,pixel405,pixel406,pixel407,pixel408,pixel409,pixel410,pixel411,pixel412,pixel413,pixel414,pixel415,pixel416,pixel417,pixel418,pixel419,pixel420,pixel421,pixel422,pixel423,pixel424,pixel425,pixel426,pixel427,pixel428,pixel429,pixel430,pixel431,pixel432,pixel433,pixel434,pixel435,pixel436,pixel437,pixel438,pixel439,pixel440,pixel441,pixel442,pixel443,pixel444,pixel445,pixel446,pixel447,pixel448,pixel449,pixel450,pixel451,pixel452,pixel453,pixel454,pixel455,pixel456,pixel457,pixel458,pixel459,pixel460,pixel461,pixel462,pixel463,pixel464,pixel465,pixel466,pixel467,pixel468,pixel469,pixel470,pixel471,pixel472,pixel473,pixel474,pixel475,pixel476,pixel477,pixel478,pixel479,pixel480,pixel481,pixel482,pixel483,pixel484,pixel485,pixel486,pixel487,pixel488,pixel489,pixel490,pixel491,pixel492,pixel493,pixel494,pixel495,pixel496,pixel497,pixel498,pixel499,pixel500,pixel501,pixel502,pixel503,pixel504,pixel505,pixel506,pixel507,pixel508,pixel509,pixel510,pixel511,pixel512,pixel513,pixel514,pixel515,pixel516,pixel517,pixel518,pixel519,pixel520,pixel521,pixel522,pixel523,pixel524,pixel525,pixel526,pixel527,pixel528,pixel529,pixel530,pixel531,pixel532,pixel533,pixel534,pixel535,pixel536,pixel537,pixel538,pixel539,pixel540,pixel541,pixel542,pixel543,pixel544,pixel545,pixel546,pixel547,pixel548,pixel549,pixel550,pixel551,pixel552,pixel553,pixel554,pixel555,pixel556,pixel557,pixel558,pixel559,pixel560,pixel561,pixel562,pixel563,pixel564,pixel565,pixel566,pixel567,pixel568,pixel569,pixel570,pixel571,pixel572,pixel573,pixel574,pixel575,pixel576,pixel577,pixel578,pixel579,pixel580,pixel581,pixel582,pixel583,pixel584,pixel585,pixel586,pixel587,pixel588,pixel589,pixel590,pixel591,pixel592,pixel593,pixel594,pixel595,pixel596,pixel597,pixel598,pixel599,pixel600,pixel601,pixel602,pixel603,pixel604,pixel605,pixel606,pixel607,pixel608,pixel609,pixel610,pixel611,pixel612,pixel613,pixel614,pixel615,pixel616,pixel617,pixel618,pixel619,pixel620,pixel621,pixel622,pixel623,pixel624,pixel625,pixel626,pixel627,pixel628,pixel629,pixel630,pixel631,pixel632,pixel633,pixel634,pixel635,pixel636,pixel637,pixel638,pixel639,pixel640,pixel641,pixel642,pixel643,pixel644,pixel645,pixel646,pixel647,pixel648,pixel649,pixel650,pixel651,pixel652,pixel653,pixel654,pixel655,pixel656,pixel657,pixel658,pixel659,pixel660,pixel661,pixel662,pixel663,pixel664,pixel665,pixel666,pixel667,pixel668,pixel669,pixel670,pixel671,pixel672,pixel673,pixel674,pixel675,pixel676,pixel677,pixel678,pixel679,pixel680,pixel681,pixel682,pixel683,pixel684,pixel685,pixel686,pixel687,pixel688,pixel689,pixel690,pixel691,pixel692,pixel693,pixel694,pixel695,pixel696,pixel697,pixel698,pixel699,pixel700,pixel701,pixel702,pixel703,pixel704,pixel705,pixel706,pixel707,pixel708,pixel709,pixel710,pixel711,pixel712,pixel713,pixel714,pixel715,pixel716,pixel717,pixel718,pixel719,pixel720,pixel721,pixel722,pixel723,pixel724,pixel725,pixel726,pixel727,pixel728,pixel729,pixel730,pixel731,pixel732,pixel733,pixel734,pixel735,pixel736,pixel737,pixel738,pixel739,pixel740,pixel741,pixel742,pixel743,pixel744,pixel745,pixel746,pixel747,pixel748,pixel749,pixel750,pixel751,pixel752,pixel753,pixel754,pixel755,pixel756,pixel757,pixel758,pixel759,pixel760,pixel761,pixel762,pixel763,pixel764,pixel765,pixel766,pixel767,pixel768,pixel769,pixel770,pixel771,pixel772,pixel773,pixel774,pixel775,pixel776,pixel777,pixel778,pixel779,pixel780,pixel781,pixel782,pixel783\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df0 = pd.read_csv('train.csv')\n",
    "\n",
    "y = df0['label']\n",
    "X = df0.drop(columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.0002, random_state=42)\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if cuda else 'cpu')\n",
    "\n",
    "X_train = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.long).to(device)\n",
    "\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.long).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.04542199894785881\n",
      "Epoch: 1, Loss: 0.042380716651678085\n",
      "Epoch: 2, Loss: 0.019898664206266403\n",
      "Epoch: 3, Loss: 0.005801439750939608\n",
      "Epoch: 4, Loss: 0.02947145327925682\n",
      "Epoch: 5, Loss: 0.0007632940541952848\n",
      "Epoch: 6, Loss: 0.0048244306817650795\n",
      "Epoch: 7, Loss: 0.003099316731095314\n",
      "Epoch: 8, Loss: 0.004508433397859335\n",
      "Epoch: 9, Loss: 0.009327090345323086\n",
      "Epoch: 10, Loss: 0.08305421471595764\n",
      "Epoch: 11, Loss: 0.022726140916347504\n",
      "Epoch: 12, Loss: 7.115956395864487e-05\n",
      "Epoch: 13, Loss: 0.000130940112285316\n",
      "Epoch: 14, Loss: 0.012190978974103928\n",
      "Epoch: 15, Loss: 0.004163279198110104\n",
      "Epoch: 16, Loss: 0.0004181701806373894\n",
      "Epoch: 17, Loss: 0.046134863048791885\n",
      "Epoch: 18, Loss: 0.0025320318527519703\n",
      "Epoch: 19, Loss: 0.006878496613353491\n",
      "Epoch: 20, Loss: 0.016284644603729248\n",
      "Epoch: 21, Loss: 0.000299790408462286\n",
      "Epoch: 22, Loss: 0.07678564637899399\n",
      "Epoch: 23, Loss: 0.0011988829355686903\n",
      "Epoch: 24, Loss: 2.4751167075010017e-05\n",
      "Epoch: 25, Loss: 0.00015674201131332666\n",
      "Epoch: 26, Loss: 8.078584505710751e-05\n",
      "Epoch: 27, Loss: 0.0009481204324401915\n",
      "Epoch: 28, Loss: 0.003358003217726946\n",
      "Epoch: 29, Loss: 0.007121769245713949\n",
      "Epoch: 30, Loss: 0.0016116822371259332\n",
      "Epoch: 31, Loss: 0.0006984819192439318\n",
      "Epoch: 32, Loss: 0.03765494003891945\n",
      "Epoch: 33, Loss: 0.00016871216939762235\n",
      "Epoch: 34, Loss: 0.005805179942399263\n",
      "Epoch: 35, Loss: 0.020171944051980972\n",
      "Epoch: 36, Loss: 4.307803465053439e-05\n",
      "Epoch: 37, Loss: 6.969092964936863e-07\n",
      "Epoch: 38, Loss: 0.34310606122016907\n",
      "Epoch: 39, Loss: 5.343564225768205e-06\n",
      "Epoch: 40, Loss: 7.293229282367975e-05\n",
      "Epoch: 41, Loss: 0.00021481933072209358\n",
      "Epoch: 42, Loss: 0.00012216871255077422\n",
      "Epoch: 43, Loss: 0.014413918368518353\n",
      "Epoch: 44, Loss: 0.006835565902292728\n",
      "Epoch: 45, Loss: 8.626357157481834e-05\n",
      "Epoch: 46, Loss: 0.003352705156430602\n",
      "Epoch: 47, Loss: 7.035344424366485e-06\n",
      "Epoch: 48, Loss: 0.00033658009488135576\n",
      "Epoch: 49, Loss: 4.854401140619302e-06\n",
      "Epoch: 50, Loss: 0.1708177626132965\n",
      "Epoch: 51, Loss: 0.0001569420419400558\n",
      "Epoch: 52, Loss: 0.0022867911029607058\n",
      "Epoch: 53, Loss: 2.52675854426343e-05\n",
      "Epoch: 54, Loss: 0.004021747503429651\n",
      "Epoch: 55, Loss: 1.0495631613594014e-05\n",
      "Epoch: 56, Loss: 3.467620990704745e-05\n",
      "Epoch: 57, Loss: 2.0304739223320212e-07\n",
      "Epoch: 58, Loss: 4.2720886995084584e-05\n",
      "Epoch: 59, Loss: 4.249022367730504e-06\n",
      "Epoch: 60, Loss: 0.0024131699465215206\n",
      "Epoch: 61, Loss: 7.520115468651056e-05\n",
      "Epoch: 62, Loss: 0.00019656485528685153\n",
      "Epoch: 63, Loss: 3.3562566386535764e-05\n",
      "Epoch: 64, Loss: 0.0003149639815092087\n",
      "Epoch: 65, Loss: 8.19549950392684e-06\n",
      "Epoch: 66, Loss: 0.023454241454601288\n",
      "Epoch: 67, Loss: 0.0030815456993877888\n",
      "Epoch: 68, Loss: 5.174399007046304e-07\n",
      "Epoch: 69, Loss: 0.0003526895307004452\n",
      "Epoch: 70, Loss: 0.0004020849009975791\n",
      "Epoch: 71, Loss: 5.999791756039485e-05\n",
      "Epoch: 72, Loss: 0.0008146875188685954\n",
      "Epoch: 73, Loss: 9.994854508477147e-07\n",
      "Epoch: 74, Loss: 1.9491171769914217e-06\n",
      "Epoch: 75, Loss: 3.7607687772833742e-06\n",
      "Epoch: 76, Loss: 3.274248956586234e-05\n",
      "Epoch: 77, Loss: 0.00010960312647512183\n",
      "Epoch: 78, Loss: 9.737854270497337e-05\n",
      "Epoch: 79, Loss: 1.796882315829862e-05\n",
      "Epoch: 80, Loss: 0.0011435821652412415\n",
      "Epoch: 81, Loss: 1.3099921147841087e-09\n",
      "Epoch: 82, Loss: 6.693904879284673e-07\n",
      "Epoch: 83, Loss: 0.01786738820374012\n",
      "Epoch: 84, Loss: 4.611925305653131e-06\n",
      "Epoch: 85, Loss: 1.2716540368273854e-05\n",
      "Epoch: 86, Loss: 0.0001216132877743803\n",
      "Epoch: 87, Loss: 2.301416316186078e-06\n",
      "Epoch: 88, Loss: 9.824905333744027e-08\n",
      "Epoch: 89, Loss: 1.7946780417332775e-07\n",
      "Epoch: 90, Loss: 9.863900913842372e-07\n",
      "Epoch: 91, Loss: 1.6812926332931966e-05\n",
      "Epoch: 92, Loss: 0.008565040305256844\n",
      "Epoch: 93, Loss: 6.078567457734607e-05\n",
      "Epoch: 94, Loss: 2.009440322581213e-06\n",
      "Epoch: 95, Loss: 6.025955912036807e-08\n",
      "Epoch: 96, Loss: 2.1761048628832214e-05\n",
      "Epoch: 97, Loss: 0.025411779060959816\n",
      "Epoch: 98, Loss: 3.536973736117943e-08\n",
      "Epoch: 99, Loss: 0.2746470272541046\n",
      "Epoch: 100, Loss: 0.0\n",
      "Epoch: 101, Loss: 2.5675623760434974e-07\n",
      "Epoch: 102, Loss: 0.025813056156039238\n",
      "Epoch: 103, Loss: 2.9343647156565567e-07\n",
      "Epoch: 104, Loss: 8.645918825322951e-08\n",
      "Epoch: 105, Loss: 0.0002724719815887511\n",
      "Epoch: 106, Loss: 2.8819792774470443e-08\n",
      "Epoch: 107, Loss: 6.549959241652914e-09\n",
      "Epoch: 108, Loss: 1.5064821923260752e-07\n",
      "Epoch: 109, Loss: 2.366941544096335e-06\n",
      "Epoch: 110, Loss: 9.812846838030964e-05\n",
      "Epoch: 111, Loss: 2.8819790998113604e-08\n",
      "Epoch: 112, Loss: 1.8339878948836486e-08\n",
      "Epoch: 113, Loss: 3.432178345974535e-05\n",
      "Epoch: 114, Loss: 1.60934760060627e-05\n",
      "Epoch: 115, Loss: 6.549959241652914e-09\n",
      "Epoch: 116, Loss: 5.571662404690869e-05\n",
      "Epoch: 117, Loss: 2.519170993764419e-05\n",
      "Epoch: 118, Loss: 0.0007105982513166964\n",
      "Epoch: 119, Loss: 0.024396546185016632\n",
      "Epoch: 120, Loss: 0.024671226739883423\n",
      "Epoch: 121, Loss: 6.757497430953663e-06\n",
      "Epoch: 122, Loss: 3.405974524639532e-08\n",
      "Epoch: 123, Loss: 3.883440513163805e-06\n",
      "Epoch: 124, Loss: 6.77770804031752e-05\n",
      "Epoch: 125, Loss: 1.2450995200197212e-05\n",
      "Epoch: 126, Loss: 0.024011703208088875\n",
      "Epoch: 127, Loss: 0.023007115349173546\n",
      "Epoch: 128, Loss: 4.1264027572651685e-07\n",
      "Epoch: 129, Loss: 0.009645529091358185\n",
      "Epoch: 130, Loss: 2.6985517820321547e-07\n",
      "Epoch: 131, Loss: 2.224343188572675e-05\n",
      "Epoch: 132, Loss: 0.006942154839634895\n",
      "Epoch: 133, Loss: 5.2497484830382746e-06\n",
      "Epoch: 134, Loss: 8.523501492163632e-06\n",
      "Epoch: 135, Loss: 0.0\n",
      "Epoch: 136, Loss: 0.0\n",
      "Epoch: 137, Loss: 6.549959241652914e-09\n",
      "Epoch: 138, Loss: 0.0\n",
      "Epoch: 139, Loss: 0.02134842239320278\n",
      "Epoch: 140, Loss: 0.0007190663600340486\n",
      "Epoch: 141, Loss: 4.341770818427904e-06\n",
      "Epoch: 142, Loss: 2.4889570227060176e-07\n",
      "Epoch: 143, Loss: 1.3099921147841087e-09\n",
      "Epoch: 144, Loss: 7.335933105423464e-08\n",
      "Epoch: 145, Loss: 1.418983538314933e-05\n",
      "Epoch: 146, Loss: 0.0\n",
      "Epoch: 147, Loss: 3.91680657685356e-07\n",
      "Epoch: 148, Loss: 0.0\n",
      "Epoch: 149, Loss: 0.01810818910598755\n",
      "Epoch: 150, Loss: 0.0\n",
      "Epoch: 151, Loss: 4.846960521831534e-08\n",
      "Epoch: 152, Loss: 4.9507110816193745e-05\n",
      "Epoch: 153, Loss: 0.0\n",
      "Epoch: 154, Loss: 2.6342018827563152e-05\n",
      "Epoch: 155, Loss: 1.911464823933784e-05\n",
      "Epoch: 156, Loss: 5.239959932623606e-08\n",
      "Epoch: 157, Loss: 5.397077984525822e-07\n",
      "Epoch: 158, Loss: 0.02081288956105709\n",
      "Epoch: 159, Loss: 1.611278861446408e-07\n",
      "Epoch: 160, Loss: 0.0\n",
      "Epoch: 161, Loss: 0.03149089589715004\n",
      "Epoch: 162, Loss: 6.549959241652914e-09\n",
      "Epoch: 163, Loss: 0.0\n",
      "Epoch: 164, Loss: 0.0\n",
      "Epoch: 165, Loss: 0.0\n",
      "Epoch: 166, Loss: 0.053752899169921875\n",
      "Epoch: 167, Loss: 3.0885275918990374e-06\n",
      "Epoch: 168, Loss: 1.2837848828439746e-07\n",
      "Epoch: 169, Loss: 0.0007448240648955107\n",
      "Epoch: 170, Loss: 0.0\n",
      "Epoch: 171, Loss: 0.08614317327737808\n",
      "Epoch: 172, Loss: 6.012700168867013e-07\n",
      "Epoch: 173, Loss: 0.0012479489669203758\n",
      "Epoch: 174, Loss: 0.0009223359520547092\n",
      "Epoch: 175, Loss: 1.4409904380840999e-08\n",
      "Epoch: 176, Loss: 0.0\n",
      "Epoch: 177, Loss: 4.5944132580189034e-05\n",
      "Epoch: 178, Loss: 2.4889827088259153e-08\n",
      "Epoch: 179, Loss: 2.8819790998113604e-08\n",
      "Epoch: 180, Loss: 0.029783256351947784\n",
      "Epoch: 181, Loss: 0.029811963438987732\n",
      "Epoch: 182, Loss: 0.01844409666955471\n",
      "Epoch: 183, Loss: 2.437626562823425e-06\n",
      "Epoch: 184, Loss: 9.169943027131922e-09\n",
      "Epoch: 185, Loss: 0.0\n",
      "Epoch: 186, Loss: 3.929975900263116e-09\n",
      "Epoch: 187, Loss: 1.3099921147841087e-09\n",
      "Epoch: 188, Loss: 0.023531025275588036\n",
      "Epoch: 189, Loss: 0.0\n",
      "Epoch: 190, Loss: 0.0\n",
      "Epoch: 191, Loss: 1.1842495950986631e-05\n",
      "Epoch: 192, Loss: 0.023599399253726006\n",
      "Epoch: 193, Loss: 0.0\n",
      "Epoch: 194, Loss: 0.02323746494948864\n",
      "Epoch: 195, Loss: 0.03448202833533287\n",
      "Epoch: 196, Loss: 0.0\n",
      "Epoch: 197, Loss: 1.3099921147841087e-09\n",
      "Epoch: 198, Loss: 3.092456381637021e-06\n",
      "Epoch: 199, Loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Train a model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 512)\n",
    "        self.fc4 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "net = Net()\n",
    "net.to(device)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS = 200\n",
    "\n",
    "net.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in range(0, len(X_train), 100):\n",
    "        X_batch = X_train[i:i+100]\n",
    "        y_batch = y_train[i:i+100]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = net(X_batch)\n",
    "        loss = F.nll_loss(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch: {epoch}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(X_test), 100):\n",
    "        X_batch = X_test[i:i+100]\n",
    "        y_batch = y_test[i:i+100]\n",
    "\n",
    "        output = net(X_batch)\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y_batch[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the submission set\n",
    "df1 = pd.read_csv('test.csv')\n",
    "df2 = pd.read_csv('submission.csv')\n",
    "\n",
    "X_submission = df1\n",
    "\n",
    "X_submission = torch.tensor(X_submission.values, dtype=torch.float32).to(device)\n",
    "\n",
    "submission = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(X_submission), 100):\n",
    "        X_batch = X_submission[i:i+100]\n",
    "        output = net(X_batch)\n",
    "        for i in output:\n",
    "            submission.append(torch.argmax(i).item())\n",
    "\n",
    "df2['Label'] = submission\n",
    "df2.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
