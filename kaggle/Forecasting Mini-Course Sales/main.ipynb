{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    country  store  product  num_sold     month       day\n",
      "id                                                       \n",
      "0       0.0    0.0     0.00  0.042000  0.083333  0.032258\n",
      "1       0.0    0.0     0.25  0.044000  0.083333  0.032258\n",
      "2       0.0    0.0     0.50  0.006000  0.083333  0.032258\n",
      "3       0.0    0.0     0.75  0.039333  0.083333  0.032258\n",
      "4       0.0    0.0     1.00  0.032667  0.083333  0.032258\n",
      "        country  store  product     month       day\n",
      "id                                                 \n",
      "136950      0.0    0.0     0.00  0.083333  0.032258\n",
      "136951      0.0    0.0     0.25  0.083333  0.032258\n",
      "136952      0.0    0.0     0.50  0.083333  0.032258\n",
      "136953      0.0    0.0     0.75  0.083333  0.032258\n",
      "136954      0.0    0.0     1.00  0.083333  0.032258\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "test = pd.read_csv('test.csv', index_col='id')\n",
    "train = pd.read_csv('train.csv', index_col='id')\n",
    "\n",
    "country_types = {'Argentina': 0, 'Canada': 1, 'Estonia': 2, 'Japan': 3, 'Spain': 4,}\n",
    "store_types = {'Kaggle Learn': 0, 'Kaggle Store': 1, 'Kagglazon': 2}\n",
    "product_types = {'Using LLMs to Improve Your Coding': 0, 'Using LLMs to Train More LLMs': 1, 'Using LLMs to Win Friends and Influence People': 2, 'Using LLMs to Win More Kaggle Competitions': 3, 'Using LLMs to Write Better': 4}\n",
    "\n",
    "train.country = [country_types[i] for i in train.country]\n",
    "train.store = [store_types[i] for i in train.store]\n",
    "train['product'] = [product_types[i] for i in train['product']]\n",
    "\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "train['month'] = train['date'].dt.month\n",
    "train['day'] = train['date'].dt.day\n",
    "train.drop('date', axis=1, inplace=True)\n",
    "\n",
    "test.country = [country_types[i] for i in test.country]\n",
    "test.store = [store_types[i] for i in test.store]\n",
    "test['product'] = [product_types[i] for i in test['product']]\n",
    "test['date'] = pd.to_datetime(test['date'])\n",
    "test['month'] = test['date'].dt.month\n",
    "test['day'] = test['date'].dt.day\n",
    "test.drop('date', axis=1, inplace=True)\n",
    "\n",
    "# normalize training data\n",
    "train['num_sold'] = train['num_sold'] / 1500\n",
    "train['country'] = train['country'] / 4\n",
    "train['store'] = train['store'] / 2\n",
    "train['product'] = train['product'] / 4\n",
    "train['month'] = train['month'] / 12\n",
    "train['day'] = train['day'] / 31\n",
    "\n",
    "# normalize testing data\n",
    "test['country'] = test['country'] / 4\n",
    "test['store'] = test['store'] / 2\n",
    "test['product'] = test['product'] / 4\n",
    "test['month'] = test['month'] / 12\n",
    "test['day'] = test['day'] / 31\n",
    "\n",
    "features = ['country', 'store', 'product', 'month', 'day']\n",
    "target = 'num_sold'\n",
    "\n",
    "X_train = train[features]\n",
    "y_train = train[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.4, random_state=42)\n",
    "\n",
    "print(train.head())\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor AdaBoost (square) training..\n",
      "Regressor AdaBoost (square) error is 4.985049226420246\n",
      "Regressor AdaBoost (linear) training..\n",
      "Regressor AdaBoost (linear) error is 7.3411327706182155\n",
      "Regressor Adaboost (exponential) training..\n",
      "Regressor Adaboost (exponential) error is 8.992186745887498\n",
      "Regressor Bagging training..\n",
      "Regressor Bagging error is 1.4153253616696202\n",
      "Regressor ExtraTrees (sq err) training..\n",
      "Regressor ExtraTrees (sq err) error is 1.475516546184739\n",
      "Regressor GradientBoosting (huber) training..\n",
      "Regressor GradientBoosting (huber) error is 1.7817152356747517\n",
      "Regressor GradientBoosting (sq err) training..\n",
      "Regressor GradientBoosting (sq err) error is 1.807616700783599\n",
      "Regressor GradientBoosting (abs err) training..\n",
      "Regressor GradientBoosting (abs err) error is 1.8423264157616404\n",
      "Regressor Random Forest (sq err) training..\n",
      "Regressor Random Forest (sq err) error is 4.905215613798217\n",
      "Regressor Random Forest (poisson) training..\n",
      "Regressor Random Forest (poisson) error is 4.905215613798217\n",
      "Regressor HistGradientBoosting (sq err) training..\n",
      "Regressor HistGradientBoosting (sq err) error is 1.1609252297994066\n",
      "Regressor HistGradientBoosting (abs err) training..\n",
      "Regressor HistGradientBoosting (abs err) error is 1.1803503872115546\n",
      "Regressor HistGradientBoosting (poisson) training..\n",
      "Regressor HistGradientBoosting (poisson) error is 1.1401973658459805\n",
      "Regressor Linear training..\n",
      "Regressor Linear error is 6.764388780914313\n",
      "Regressor Ridge (Linear) training..\n",
      "Regressor Ridge (Linear) error is 6.7642451817190254\n",
      "Regressor RidgeCV training..\n",
      "Regressor RidgeCV error is 6.764245180369649\n",
      "Regressor SGDRegressor (elasticnet) training..\n",
      "Regressor SGDRegressor (elasticnet) error is 6.804904450273415\n",
      "Regressor SGDRegressor (l2) training..\n",
      "Regressor SGDRegressor (l2) error is 6.722414486270474\n",
      "Regressor SGDRegressor (l1) training..\n",
      "Regressor SGDRegressor (l1) error is 6.792365115852743\n",
      "Regressor Elastic Net (random) training..\n",
      "Regressor Elastic Net (random) error is 9.084854717752485\n",
      "Regressor Elastic Net (cyclic) training..\n",
      "Regressor Elastic Net (cyclic) error is 9.084854717752485\n",
      "Regressor ARD training..\n",
      "Regressor ARD error is 6.7647831133424425\n",
      "Regressor BayesianRidge training..\n",
      "Regressor BayesianRidge error is 6.764230102916831\n",
      "Regressor RANSAC training..\n",
      "Regressor RANSAC error is 8.173747823076988\n",
      "Regressor TheilSenRegressor training..\n",
      "Regressor TheilSenRegressor error is 6.499018518840108\n",
      "Regressor PoissonRegressor training..\n",
      "Regressor PoissonRegressor error is 9.003688238730124\n",
      "Regressor TweedieRegressor (auto) training..\n",
      "Regressor TweedieRegressor (auto) error is 8.453185148147764\n",
      "Regressor TweedieRegressor (identity) training..\n",
      "Regressor TweedieRegressor (identity) error is 8.453185148147764\n",
      "Regressor TweedieRegressor (log) training..\n",
      "Regressor TweedieRegressor (log) error is 9.084590249937461\n",
      "Regressor GammaRegressor training..\n",
      "Regressor GammaRegressor error is 8.16345249853027\n",
      "Regressor PassiveAggressiveRegressor (epsilon_insensitive) training..\n",
      "Regressor PassiveAggressiveRegressor (epsilon_insensitive) error is 9.131999880527754\n",
      "Regressor PassiveAggressiveRegressor (squared_epsilon_insensitive) training..\n",
      "Regressor PassiveAggressiveRegressor (squared_epsilon_insensitive) error is 8.63221159617996\n",
      "Regressor KNeighbors training..\n",
      "Regressor KNeighbors error is 1.3755360837288546\n",
      "Regressor Radius Neighbors training..\n",
      "Regressor Radius Neighbors error is 6.74569201971735\n",
      "Regressor MLP training..\n",
      "Regressor MLP error is 1.895804207824911\n",
      "Regressor DecisionTree training..\n",
      "Regressor DecisionTree error is 1.4771397306397307\n",
      "Regressor Extra Tree training..\n",
      "Regressor Extra Tree error is 1.4782082674130865\n",
      "Regressor Linear SVR (epsilon_insensitive) training..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor Linear SVR (epsilon_insensitive) error is 6.430908211784199\n",
      "Regressor Linear SVR (squared_epsilon_insensitive) training..\n",
      "Regressor Linear SVR (squared_epsilon_insensitive) error is 6.764375284714902\n",
      "Regressor SVR training..\n",
      "Regressor SVR error is 8.284432011219618\n",
      "Regressor LassoLarsIC (bic) training..\n",
      "Regressor LassoLarsIC (bic) error is 6.764388780914205\n",
      "Regressor LassoLarsIC (aic) training..\n",
      "Regressor LassoLarsIC (aic) error is 6.764388780914205\n",
      "Regressor PLS training..\n",
      "Regressor PLS error is 6.764382193587918\n",
      "Regressor OrthogonalMatchingPursuit training..\n",
      "Regressor OrthogonalMatchingPursuit error is 6.918958285651764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:116: FutureWarning: 'normalize' was deprecated in version 1.2 and will be removed in 1.4. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:116: FutureWarning: 'normalize' was deprecated in version 1.2 and will be removed in 1.4. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/beanbeah/ML/blob/main/sklearn-ml-bruteforce.py\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.linear_model import LassoLarsIC, GammaRegressor, TweedieRegressor, BayesianRidge, ARDRegression,  LinearRegression, Ridge, RidgeCV, SGDRegressor, ElasticNet, RANSACRegressor, TheilSenRegressor, PoissonRegressor, PassiveAggressiveRegressor, OrthogonalMatchingPursuit\n",
    "from sklearn.neighbors import KNeighborsRegressor, RadiusNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "regressors = {\n",
    "    \"AdaBoost (square)\" : AdaBoostRegressor(random_state=0, n_estimators=100, loss=\"square\"),\n",
    "    \"AdaBoost (linear)\" : AdaBoostRegressor(random_state=0, n_estimators=100, loss=\"linear\"),\n",
    "    \"Adaboost (exponential)\" : AdaBoostRegressor(random_state=0, n_estimators=100, loss=\"exponential\"),\n",
    "    \"Bagging\" : BaggingRegressor(n_estimators=10, random_state=0),\n",
    "    \"ExtraTrees (sq err)\" : ExtraTreesRegressor(criterion = \"squared_error\", n_estimators=100, random_state=0),\n",
    "    \"GradientBoosting (huber)\" : GradientBoostingRegressor(random_state=0,loss=\"huber\"),\n",
    "    \"GradientBoosting (sq err)\" : GradientBoostingRegressor(random_state=0,loss=\"squared_error\"),\n",
    "    \"GradientBoosting (abs err)\" : GradientBoostingRegressor(random_state=0,loss=\"absolute_error\"),\n",
    "    \"Random Forest (sq err)\" : RandomForestRegressor(max_depth=2, random_state=0,criterion=\"squared_error\"),\n",
    "    \"Random Forest (poisson)\" : RandomForestRegressor(max_depth=2, random_state=0,criterion=\"poisson\"),\n",
    "    \"HistGradientBoosting (sq err)\" : HistGradientBoostingRegressor(loss=\"squared_error\"),\n",
    "    \"HistGradientBoosting (abs err)\" : HistGradientBoostingRegressor(loss=\"absolute_error\"),\n",
    "    \"HistGradientBoosting (poisson)\" : HistGradientBoostingRegressor(loss=\"poisson\"),\n",
    "    \"Linear\" : LinearRegression(),\n",
    "    \"Ridge (Linear)\" : Ridge(),\n",
    "    \"RidgeCV\" : RidgeCV(),\n",
    "    \"SGDRegressor (elasticnet)\" : make_pipeline(StandardScaler(),SGDRegressor(max_iter=1000, tol=1e-3,penalty=\"elasticnet\")),\n",
    "    \"SGDRegressor (l2)\" : make_pipeline(StandardScaler(),SGDRegressor(max_iter=1000, tol=1e-3,penalty=\"l2\")),\n",
    "    \"SGDRegressor (l1)\" : make_pipeline(StandardScaler(),SGDRegressor(max_iter=1000, tol=1e-3,penalty=\"l1\")),\n",
    "    \"Elastic Net (random)\" : ElasticNet(random_state=0,selection=\"random\"),\n",
    "    \"Elastic Net (cyclic)\" : ElasticNet(random_state=0,selection=\"cyclic\"),\n",
    "    \"ARD\" : ARDRegression(),\n",
    "    \"BayesianRidge\" : BayesianRidge(),\n",
    "    \"RANSAC\": RANSACRegressor(random_state=0),\n",
    "    \"TheilSenRegressor\" : TheilSenRegressor(random_state=0),\n",
    "    \"PoissonRegressor\" : PoissonRegressor(),\n",
    "    \"TweedieRegressor (auto)\" : TweedieRegressor(link=\"auto\"),\n",
    "    \"TweedieRegressor (identity)\" : TweedieRegressor(link=\"identity\"),\n",
    "    \"TweedieRegressor (log)\" : TweedieRegressor(link=\"log\"),\n",
    "    \"GammaRegressor\" : GammaRegressor(),\n",
    "    \"PassiveAggressiveRegressor (epsilon_insensitive)\" :  PassiveAggressiveRegressor(max_iter=100, random_state=0, tol=1e-3, loss=\"epsilon_insensitive\"),\n",
    "    \"PassiveAggressiveRegressor (squared_epsilon_insensitive)\" :  PassiveAggressiveRegressor(max_iter=100, random_state=0, tol=1e-3, loss=\"squared_epsilon_insensitive\"),\n",
    "    \"KNeighbors\" : KNeighborsRegressor(n_neighbors=3),\n",
    "    \"Radius Neighbors\" : RadiusNeighborsRegressor(radius=1.0),\n",
    "    \"MLP\" : MLPRegressor(random_state=1, max_iter=500),\n",
    "    \"DecisionTree\" : DecisionTreeRegressor(random_state=0),\n",
    "    \"Extra Tree\" : ExtraTreeRegressor(random_state=0),\n",
    "    \"Linear SVR (epsilon_insensitive)\" : make_pipeline(StandardScaler(), LinearSVR(random_state=0, tol=1e-5, loss=\"epsilon_insensitive\")),\n",
    "    \"Linear SVR (squared_epsilon_insensitive)\" : make_pipeline(StandardScaler(), LinearSVR(random_state=0, tol=1e-5, loss=\"squared_epsilon_insensitive\")),\n",
    "    \"SVR\" : make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2)),\n",
    "    \"LassoLarsIC (bic)\" : LassoLarsIC(criterion='bic', normalize=False),\n",
    "    \"LassoLarsIC (aic)\" : LassoLarsIC(criterion='aic', normalize=False),\n",
    "    \"PLS\" : PLSRegression(n_components=2),\n",
    "    \"OrthogonalMatchingPursuit\" : OrthogonalMatchingPursuit(),\n",
    "}\n",
    "\n",
    "def test_regressor(regressor_type, X_train, X_test, y_train, y_test):\n",
    "    print(\"Regressor\", regressor_type, \"training..\")\n",
    "    reg = regressors[regressor_type]\n",
    "    reg.fit(X_train, y_train)\n",
    "    y_predicted = reg.predict(X_test)\n",
    "    err = metrics.mean_absolute_error(y_test, y_predicted) * 100\n",
    "    pickle.dump(\n",
    "        reg, open(f'trainedmodels/{regressor_type}.pkl', 'wb'))\n",
    "    print(\"Regressor\", regressor_type, \"error is\", err)\n",
    "    return [regressor_type, err]\n",
    "\n",
    "def test_regressors(X_train, X_test, y_train, y_test):\n",
    "    result_queue = []\n",
    "    multiple_results = [\n",
    "        (test_regressor(key, X_train, X_test, y_train, y_test)) for key in regressors]\n",
    "    for res in multiple_results:\n",
    "        if res:\n",
    "            try:\n",
    "                tmp = res[0]\n",
    "                if tmp is not None:\n",
    "                    result_queue.append(tmp)\n",
    "            except TimeoutError:\n",
    "                print(\"\\nClassifier\", res[1], \"exceeded the time limit.\")\n",
    "            except MemoryError:\n",
    "                print(\"\\nClassifier\", res[1], \"exceeded the memory limit.\")\n",
    "\n",
    "    mae = {}\n",
    "    for value in multiple_results:\n",
    "        mae[value[0]] = value[1]\n",
    "    mae = {k: v for k, v in sorted(\n",
    "        mae.items(), key=lambda item: item[1], reverse=False)}\n",
    "    returning = \"\"\n",
    "    returning += \"---\"*20\n",
    "    returning += \"\\nResults (smaller error better): \"\n",
    "    i = 1\n",
    "    for key in mae:\n",
    "        returnin = \"\\n\" + (str(i).zfill(2) + ' ' + key + ' ' +\n",
    "                    '{:.2f}'.format(mae[key]) + '%')\n",
    "        returning += returnin\n",
    "        i += 1\n",
    "    returning += \"\\n\" + \"---\"*20\n",
    "    return returning\n",
    "\n",
    "output = test_regressors(X_train, X_test, y_train, y_test)\n",
    "with open(\"trainedmodels/results.txt\", \"w\") as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trainedmodels/HistGradientBoosting (poisson).pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "test_results = model.predict(test) * 1500\n",
    "\n",
    "output = pd.DataFrame({'id': test.index, 'num_sold': test_results})\n",
    "\n",
    "output['num_sold'] = output['num_sold'].astype('int')\n",
    "\n",
    "output.to_csv('submission3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
